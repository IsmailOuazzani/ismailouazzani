{"/atomicai/":{"data":{"":"","#":"I built AtomicAI, an AI that plays Atomic Chess, a variant of regular chess where captured pieces explode. The goal was to create a deep learning-based evaluation function that rates board positions and helps a chess engine make better moves.\nThe Challenge Unlike standard chess, Atomic Chess has unique tactical patterns, and there is limited domain knowledge available for designing an evaluation function. Traditional engines rely on handcrafted heuristics, but our approach leverages deep learning to learn position strength directly from game data.\nApproach Collected 900,000+ games from the Lichess database, filtering only high-level (Elo 2000+) matches to ensure strategic depth. We then extracted 2 million board states for training. Designed a CNN-based evaluation function that assesses a given board position and assigns a value between 0 (winning for Black) and 1 (winning for White). The model learns spatial relationships between pieces, making it more adaptable than handcrafted heuristics. We integrated our AI with Andoma, a Python-based chess engine, allowing it to play complete games against humans and other AI opponents Code View on Github Project Report "},"title":"Automic Chess AI"},"/blog/":{"data":{"":" "},"title":"Blog"},"/projects/":{"data":{"":" Automating Robot Success Classification with Vision-Language Models Atomic Chess AI Pick and Place with a Kuka Robot rm "},"title":"Projects"},"/projects/kukapickandplace/":{"data":{"":"","#":"Introduction During our labs in ECE470: Robot Modeling and Control, my classmates and I tackled motion planning and obstacle avoidance using the KUKA robotic arm. Our goal was to have the KUKA robotic arm pick up an object and drop it into a cup while avoiding obstacles. We utilized an artificial potential field (APF) algorithm, which combines attractive forces (pulling the arm toward its goal) and repulsive forces (pushing it away from obstacles). The lab also gave us the opportunity to experiment with different parameters and refine our algorithm to handle real-world complexities.\nApproach The robot followed a predefined trajectory, ensuring it avoided obstacles and successfully grasped the object. Using inverse kinematics, we calculated the necessary joint angles and tested our motion plan in simulation before applying it to the physical robot. Using Matlab, we tested our repulsive function to ensure it generated the expected values. Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player! Demo "},"title":"Pick and Place with a Kuka Robot Arm"},"/robosuccessvlm/":{"data":{"":"","#":"Robots performing real-world tasks often require human annotations to determine whether a task was completed successfully. This slows down development cycles and limits scalability. We explored an AI-driven approach to classify robot demonstration success using only video data and natural language instructions.\nOur Approach We fine-tuned InternVL2 Vision-Language Models on a subset of the Droid dataset, which contains 76,000+ robotic demonstrations across 86 tasks and 564 unique environments. Instead of using predefined heuristics, our model learns from video and language context to determine whether a task was completed correctly.\nResults Our findings show that Vision-Language Models can autonomously evaluate robot performance, reducing reliance on human labels and accelerating robotic learning and deployment. Future work includes temporal localization (pinpointing success/failure moments in video) and adapting models for diverse robotic platforms.\nCode View on Github Project Report "},"title":"Automating Robot Success Classification with Vision-Language Models"}}