{"/blog/":{"data":{"":" "},"title":"Blog"},"/projects/":{"data":{"attempted-startups#Attempted Startups":"Attempted Startups Forestify: Help farmers recover quickly when weather ruins their crops Splatchat: Messaging app for 3d photos ","projects#Projects":" Automating Robot Success Classification with Vision-Language Models Atomic Chess AI Pick and Place with a Kuka Robot Arm "},"title":"_index"},"/projects/atomicai/":{"data":{"":"","#":"Introduction I built AtomicAI, an AI that plays Atomic Chess, a variant of regular chess where captured pieces explode. The goal was to create a deep learning-based evaluation function that rates board positions and helps a chess engine make better moves.\nThe Challenge Unlike standard chess, Atomic Chess has unique tactical patterns, and there is limited domain knowledge available for designing an evaluation function. Traditional engines rely on handcrafted heuristics, but our approach leverages deep learning to learn position strength directly from game data.\nApproach Collected 900,000+ games from the Lichess database, filtering only high-level (Elo 2000+) matches to ensure strategic depth. We then extracted 2 million board states for training. Designed a CNN-based evaluation function that assesses a given board position and assigns a value between 0 (winning for Black) and 1 (winning for White). The model learns spatial relationships between pieces, making it more adaptable than handcrafted heuristics. We integrated our AI with Andoma, a Python-based chess engine, allowing it to play complete games against humans and other AI opponents Code View on Github Project Report "},"title":"Atomic Chess AI"},"/projects/forestify/":{"data":{"":"","#":"Introduction I worked on this startup as part of the Hatchery (Summer 2023), a startup incubator at the University of Toronto. In this program, undergrad students team up with MBA students and mentors from diverse fields, and we pitched every other week to refine our idea iteratively.\nThis idea evolved through multiple pivots. We started with drone reforestation, shifted to controlling invasive species, and eventually became interested in farming. During a discussion with US farmers, we learned about a major flaw in traditional insurance.\nAfter a meteorological event devastated a significant portion of crops, it could take months for insurers to assess damage and pay out claims - a delay that poses serious risks for farmers. We explored ways to shorten this process using parametric insurance. Our solution aimed to help parametric insurance companies access reliable data through a network of sensors.\nPitch Deck Why It Didn’t Work The co-founders had divergent expectations, goals, and aspirations. Our team was dysfunctional, which ultimately proved to be our biggest challenge. Additionally, we lacked experience with insurance legislation. We also spent too much time honing our business plan and too little working towards an MVP."},"title":"Forestify"},"/projects/kukapickandplace/":{"data":{"":"","#":"Introduction During our labs in ECE470: Robot Modeling and Control, my classmates and I tackled motion planning and obstacle avoidance using the KUKA robotic arm. Our goal was to have the KUKA robotic arm pick up an object and drop it into a cup while avoiding obstacles. We utilized an artificial potential field (APF) algorithm, which combines attractive forces (pulling the arm toward its goal) and repulsive forces (pushing it away from obstacles). The lab also gave us the opportunity to experiment with different parameters and refine our algorithm to handle real-world complexities.\nApproach The robot followed a predefined trajectory, ensuring it avoided obstacles and successfully grasped the object. Using inverse kinematics, we calculated the necessary joint angles and tested our motion plan in simulation before applying it to the physical robot. Using Matlab, we tested our repulsive function to ensure it generated the expected values. Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player! Demo Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player! Code View on Github "},"title":"Pick and Place with a Kuka Robot Arm"},"/projects/robosuccessvlm/":{"data":{"":"","#":"Introduction Robots performing real-world tasks often require human annotations to determine whether a task was completed successfully. This slows down development cycles and limits scalability. We explored an AI-driven approach to classify robot demonstration success using only video data and natural language instructions.\nOur Approach We fine-tuned InternVL2 Vision-Language Models on a subset of the Droid dataset, which contains 76,000+ robotic demonstrations across 86 tasks and 564 unique environments. Instead of using predefined heuristics, our model learns from video and language context to determine whether a task was completed correctly.\nYour browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player! Results Our findings show that Vision-Language Models can autonomously evaluate robot performance, reducing reliance on human labels and accelerating robotic learning and deployment. Future work includes temporal localization (pinpointing success/failure moments in video) and adapting models for diverse robotic platforms.\nCode View on Github Project Report "},"title":"Automating Robot Success Classification with Vision-Language Models"},"/projects/splatchat/":{"data":{"":"","#":"Introduction I got inspired after using Niantic’s Scaniverse, an app that captures 3D scenes by filming around using gaussian splatting, a cutting-edge 3D reconstruction technique. My main observation was that if people truly enjoyed these “splats,” they’d prefer sharing them with friends and family rather than broadcasting on public profiles or feeds. In essence, I wanted to create the WhatsApp of 3D pictures, with Scaniverse as the Instagram counterpart.\nWith that idea in mind, I took a “build it first” approach. I hacked together a prototype in just three weeks and then invited my friends to test it. Typically, I focus on back-end, so this was my first time building a front-end for iOS and Android, as well as deploying my own API in the cloud.\nTech Stack React Native: Used for building the mobile app for both iOS and Android. FastAPI: Handles the API and authentication. Google Cloud Compute: Hosts the API. Nginx: Manages routing and domain names. Docker: Packages the environment with all the Gaussian splatting dependencies. Runpod: Provides serverless GPU support to turn videos into gaussian splats. Gsplat: The tool that performs Gaussian splatting. Demo Your browser doesn't support embedded videos, but don't worry, you can download it and watch it with your favorite video player! Why it didn’t work I struggled to pinpoint a compelling use case. Even my own friends weren’t motivated to use it. Capturing videos of scenes that are inherently static proved inconvenient, and ultimately, the concept didn’t generate enough interest."},"title":"Splatchat"}}